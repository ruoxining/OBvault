## Survey
### A Survey of the State  of Explainable AI for Natural Language Processing
This survey thoroughly explains the state of explainable NLP. The *Introduction* discusses two distinguishing criteria for explanability models (1) whether the explanation is for each prediction individually or the modelâ€™s prediction process as a whole, and (2) determining whether generating the explanation requires post-processing or not. In *Categorization of Explanations*, this paper categorizes the explanation models into local (provides information or justification for the model's prediction on a specific input) vs. global (provides similar justification by revealing how the model's predictive process works, independently of any particular input), and self-explaining (also directly interpretable, generates the explanation at the same time as the prediction, e.g. decision trees, rule-based models, and feature saliency models like attention models) vs. post-hoc (an additional operation is performed after the predictions are made). This section also states that the different categories of models can overlap. In section *Aspects of Explanations*, this paper introduces three types of explanation techniques: (1) explainability techniques (feature importance, surrogate model, example-driven, provenance-based, declarative induction), (2) operations to enable explainability (first-derivation saliency, layer-wise relevance propagation, and input perturbations, attention, LSTM gating signals, explainability-aware architecture design) and (3) visualization techniques (saliency, raw declarative representations, natural language explanation). The section *Evaluation* introduces several evaluating metrices. 