
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://ruoxining.github.io/OBvault/DL_Notes/NLP_Theory/Embodied%20AI/">
      
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.3, mkdocs-material-9.1.18">
    
    
      
        <title>Embodied AI - MiniBabelLibrary</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.26e3688c.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ecc896b0.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="black" data-md-color-accent="indigo">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#1-the-birth-of-an-agent-construction-of-llm-based-agents" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../.." title="MiniBabelLibrary" class="md-header__button md-logo" aria-label="MiniBabelLibrary" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MiniBabelLibrary
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Embodied AI
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/ruoxining/OBvault" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    OBvault
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="MiniBabelLibrary" class="md-nav__button md-logo" aria-label="MiniBabelLibrary" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    MiniBabelLibrary
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/ruoxining/OBvault" title="前往仓库" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.4.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    OBvault
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
      
      
      
        <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
          申请心得
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          申请心得
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Application/" class="md-nav__link">
        索引
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Application/%E3%80%90TODO%E3%80%91%E6%88%91%E7%9A%8424fall%E7%94%B3%E8%AF%B7%E8%AE%B0%E5%BD%95/" class="md-nav__link">
        24fall申请记录
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Application/ZJU%20English%20Major%20to%20CS%26NLP/" class="md-nav__link">
        英专转CS&NLP指北
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Application/NLP%20Global%20PHD%20Equality%20Digest/" class="md-nav__link">
        搬运NLP申请PhD经验
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
          CS专栏
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          CS专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/" class="md-nav__link">
        索引
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
      
      
      
        <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="0">
          C++持续进步
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          C++持续进步
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E5%90%AC%E8%AF%BE%EF%BC%88ZJU%20%26%20Stanford%20CS106B%EF%BC%89/" class="md-nav__link">
        ZJU听课
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/ZJU%20%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" class="md-nav__link">
        ZJU期末复习
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E7%BB%A7%E6%89%BF%20Inheritance/" class="md-nav__link">
        继承
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E6%A8%A1%E6%9D%BF%E5%92%8C%E6%A0%87%E5%87%86%E6%A8%A1%E6%9D%BF%E5%BA%93%20Template%20%26%20STL/" class="md-nav__link">
        模板和标准模板库
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E7%B1%BB%E5%9E%8B%E5%92%8C%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2Type%20%26%20Type%20Conversion/" class="md-nav__link">
        类型和类型转换
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E6%96%87%E4%BB%B6%E8%AF%BB%E5%86%99%20File%20IO/" class="md-nav__link">
        文件读写
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E6%B5%85%E6%8B%B7%E8%B4%9D%E5%92%8C%E6%B7%B1%E6%8B%B7%E8%B4%9D%20Shallow%20copy%20%26%20Deep%20copy/" class="md-nav__link">
        浅拷贝和深拷贝
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E6%88%91%E7%9A%84C%2B%2B%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/%E5%91%BD%E5%90%8D%E7%A9%BA%E9%97%B4%20Namespace/" class="md-nav__link">
        命名空间
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_3" >
      
      
      
        <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0">
          OS操作系统
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_3_3">
          <span class="md-nav__icon md-icon"></span>
          OS操作系统
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/OS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/" class="md-nav__link">
        00-Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/OS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Lecture_Overview/" class="md-nav__link">
        0-引入
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/OS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E8%BF%9B%E7%A8%8B%E4%B8%8E%E5%90%8C%E6%AD%A5/" class="md-nav__link">
        1-进程与同步
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/OS%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/%E5%86%85%E5%AD%98_%E5%AD%98%E5%82%A8_%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/" class="md-nav__link">
        2-内存/存储/文件系统
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/8086%2680386%E6%B1%87%E7%BC%96/" class="md-nav__link">
        8086&80386汇编
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/%E7%BC%96%E7%A8%8B%E8%A7%84%E8%8C%83%E6%96%87%E6%A1%A3%E6%A8%A1%E6%9D%BF/" class="md-nav__link">
        编程规范文档模板
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/C%E8%AF%AD%E8%A8%80/" class="md-nav__link">
        C语言
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/C%E5%A4%A7%E7%A8%8Blibgraphics%E8%B8%A9%E5%9D%91%E6%96%87%E6%A1%A3/" class="md-nav__link">
        C大程libgraphics踩坑文档
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../CS_Notes/Crawler%F0%9F%95%B7/" class="md-nav__link">
        爬虫
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          DL专栏
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          DL专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        索引
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_2" >
      
      
      
        <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
          NLP Theory
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_2">
          <span class="md-nav__icon md-icon"></span>
          NLP Theory
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Chain-of-Thought.md" class="md-nav__link">
        CoT prompt
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Contrastive%20Learning%20and%20Interpretability/" class="md-nav__link">
        Contrastive Learning and Interpretability
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_3" >
      
      
      
        <label class="md-nav__link" for="__nav_4_3" id="__nav_4_3_label" tabindex="0">
          NLP Tech
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_3">
          <span class="md-nav__icon md-icon"></span>
          NLP Tech
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP_Tech/%E8%A7%84%E5%88%99%E6%8F%90%E5%8F%96%E6%96%87%E6%9C%AC%E6%A0%BC%E5%BC%8F%E7%BB%8F%E9%AA%8C%E6%80%BB%E7%BB%93/" class="md-nav__link">
        规则提取文本格式经验总结
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_4" >
      
      
      
        <label class="md-nav__link" for="__nav_4_4" id="__nav_4_4_label" tabindex="0">
          Python用法速查
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_4_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_4">
          <span class="md-nav__icon md-icon"></span>
          Python用法速查
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python_Usage/IPDB/" class="md-nav__link">
        IPDB
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python_Usage/Pandas%20Dataframe%E5%B8%B8%E7%94%A8/" class="md-nav__link">
        Pandas DF速查
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python_Usage/Python%E7%9A%84%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7/" class="md-nav__link">
        Python类速查
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Python_Usage/Pytorch 常用命令.md" class="md-nav__link">
        Pytorch速查
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4_5" >
      
      
      
        <label class="md-nav__link" for="__nav_4_5" id="__nav_4_5_label" tabindex="0">
          命令行用法速查
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_4_5">
          <span class="md-nav__icon md-icon"></span>
          命令行用法速查
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Terminal_Usage/Anaconda%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/" class="md-nav__link">
        Anaconda速查
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Terminal_Usage/Linux/" class="md-nav__link">
        Linux命令行速查
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/DL_Notes/Terminal_Usage/tmux.md" class="md-nav__link">
        tmux速查
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
      
      
      
        <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
          Linguistics专栏
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Linguistics专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_1" >
      
      
      
        <label class="md-nav__link" for="__nav_5_1" id="__nav_5_1_label" tabindex="0">
          Phonetics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_1_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_1">
          <span class="md-nav__icon md-icon"></span>
          Phonetics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics_Notes/Phonetics/Reduction%20of%20Intentional%20verb%2Bprep/" class="md-nav__link">
        Reduction in verb.+prep.
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics_Notes/Phonetics/Signal%20Basics/" class="md-nav__link">
        Signal Basics
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
      
      
      
        <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
          Syntax
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          Syntax
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics/转换生成句法.md" class="md-nav__link">
        转换生成句法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics_Notes/Syntax/Chomsky%E2%80%99s%20Universal%20Grammar/" class="md-nav__link">
        乔姆斯基普遍语法
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../docs/Linguistics_Notes/Syntax/UD and SUD.md" class="md-nav__link">
        UD和SUD
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
      
      
      
        <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
          Semantics
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_5_3">
          <span class="md-nav__icon md-icon"></span>
          Semantics
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics_Notes/Semantics/" class="md-nav__link">
        0-Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics_Notes/Semantics/Definition%20Clearification/" class="md-nav__link">
        1-Definition Clearification
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics_Notes/Semantics/Logics%20%26%20Formal%20Semantics/" class="md-nav__link">
        2-Logics & Formal Semantics
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Linguistics_Notes/Semantics/Scope%20Ambiguity/" class="md-nav__link">
        3-Scope Ambiguity
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
      
      
      
        <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
          Math专栏
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          Math专栏
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Math_Notes/" class="md-nav__link">
        索引
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Math_Notes/Graph%20Theory/Laplacian%20Matrix/" class="md-nav__link">
        图论
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Math_Notes/%E9%9B%B6%E6%95%A3%E7%9A%84%E9%A2%98/" class="md-nav__link">
        零散的题
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
      
      
      
        <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0">
          其它课程
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_7">
          <span class="md-nav__icon md-icon"></span>
          其它课程
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Other_Courses/" class="md-nav__link">
        索引
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../Other_Courses/%E6%89%93%E5%BC%80%E8%89%BA%E6%9C%AF%E4%B9%8B%E9%97%A8-%E9%92%A2%E7%90%B4%E6%9C%9F%E6%9C%AB%E5%A4%8D%E4%B9%A0/" class="md-nav__link">
        钢琴期末复习
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_8" >
      
      
      
        <label class="md-nav__link" for="__nav_8" id="__nav_8_label" tabindex="0">
          瞎说一些东西
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_8_label" aria-expanded="false">
        <label class="md-nav__title" for="__nav_8">
          <span class="md-nav__icon md-icon"></span>
          瞎说一些东西
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E7%9E%8E%E8%AF%B4%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF/" class="md-nav__link">
        索引
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E7%9E%8E%E8%AF%B4%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF/%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E7%9A%84%E5%B7%A5%E5%85%B7/" class="md-nav__link">
        杂七杂八的工具
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E7%9E%8E%E8%AF%B4%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF/Notion%E5%92%8CObsidian%EF%BC%8C%E9%80%89%E5%93%AA%E4%B8%80%E4%B8%AA/" class="md-nav__link">
        Notion vs. Obsidian
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../../%E7%9E%8E%E8%AF%B4%E4%B8%80%E4%BA%9B%E4%B8%9C%E8%A5%BF/ZOTERO%20%E7%AE%80%E4%BB%8B%E5%8F%8A%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B/" class="md-nav__link">
        ZOTERO简介
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-the-birth-of-an-agent-construction-of-llm-based-agents" class="md-nav__link">
    1. The Birth of An Agent: Construction of LLM-based Agents
  </a>
  
    <nav class="md-nav" aria-label="1. The Birth of An Agent: Construction of LLM-based Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-brain-primarily-composed-of-an-llm" class="md-nav__link">
    1.1 Brain: Primarily Composed of An LLM
  </a>
  
    <nav class="md-nav" aria-label="1.1 Brain: Primarily Composed of An LLM">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-natural-language-interaction" class="md-nav__link">
    1.1.1 Natural Language Interaction
  </a>
  
    <nav class="md-nav" aria-label="1.1.1 Natural Language Interaction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#high-quality-generation" class="md-nav__link">
    High-quality generation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-understanding" class="md-nav__link">
    Deep understanding
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-knowledge" class="md-nav__link">
    1.1.2 Knowledge
  </a>
  
    <nav class="md-nav" aria-label="1.1.2 Knowledge">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pretrain-model" class="md-nav__link">
    Pretrain model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linguistic-knowledge" class="md-nav__link">
    Linguistic knowledge
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#commonsense-knowledge" class="md-nav__link">
    Commonsense knowledge
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#actionable-knowledge" class="md-nav__link">
    Actionable knowledge
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#potential-issues-of-knowledge" class="md-nav__link">
    Potential issues of knowledge
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-memory" class="md-nav__link">
    1.1.3 Memory
  </a>
  
    <nav class="md-nav" aria-label="1.1.3 Memory">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#memory-capability" class="md-nav__link">
    Memory capability
  </a>
  
    <nav class="md-nav" aria-label="Memory capability">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#raising-the-length-limit-of-transformers" class="md-nav__link">
    Raising the length limit of Transformers
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#summarizing-memory" class="md-nav__link">
    Summarizing memory
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compressing-memories-with-vectors-or-data-structures" class="md-nav__link">
    Compressing memories with vectors or data structures
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#memory-retrieval" class="md-nav__link">
    Memory retrieval
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#114-reasoning-planning" class="md-nav__link">
    1.1.4 Reasoning &amp; Planning
  </a>
  
    <nav class="md-nav" aria-label="1.1.4 Reasoning & Planning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reasoning" class="md-nav__link">
    Reasoning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#planning" class="md-nav__link">
    Planning
  </a>
  
    <nav class="md-nav" aria-label="Planning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#plan-formulation" class="md-nav__link">
    Plan formulation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#plan-reflection" class="md-nav__link">
    Plan reflection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#115-transferability-and-generalization" class="md-nav__link">
    1.1.5 Transferability and Generalization
  </a>
  
    <nav class="md-nav" aria-label="1.1.5 Transferability and Generalization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unseen-task-generalization" class="md-nav__link">
    Unseen task generalization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#in-context-learning" class="md-nav__link">
    In-context learning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#continual-learning" class="md-nav__link">
    Continual learning
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-perception-multimodal-inputs-for-llm-based-agents" class="md-nav__link">
    1.2 Perception: Multimodal Inputs for LLM-based Agents
  </a>
  
    <nav class="md-nav" aria-label="1.2 Perception: Multimodal Inputs for LLM-based Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#121-visual" class="md-nav__link">
    1.2.1 Visual
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#122-audio" class="md-nav__link">
    1.2.2 Audio
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-action-expand-action-space-of-llm-based-agents" class="md-nav__link">
    1.3 Action: Expand Action Space of LLM-based Agents
  </a>
  
    <nav class="md-nav" aria-label="1.3 Action: Expand Action Space of LLM-based Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#131-tool-using" class="md-nav__link">
    1.3.1 Tool Using
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#132-embodied-action" class="md-nav__link">
    1.3.2 Embodied Action
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-agents-in-practice-applications-of-llm-based-agents" class="md-nav__link">
    2. Agents in Practice: Applications of LLM-based Agents
  </a>
  
    <nav class="md-nav" aria-label="2. Agents in Practice: Applications of LLM-based Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-general-ability-of-single-agent" class="md-nav__link">
    2.1 General Ability of Single Agent
  </a>
  
    <nav class="md-nav" aria-label="2.1 General Ability of Single Agent">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#211-task-orietned-deployment" class="md-nav__link">
    2.1.1 Task-orietned Deployment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#212-innovation-oriented-deployment" class="md-nav__link">
    2.1.2 Innovation-oriented Deployment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#213-lifecycle-oriented-deployment" class="md-nav__link">
    2.1.3 Lifecycle-oriented Deployment
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-coordinating-potential-of-multiple-agents" class="md-nav__link">
    2.2 Coordinating Potential of Multiple Agents
  </a>
  
    <nav class="md-nav" aria-label="2.2 Coordinating Potential of Multiple Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#221-cooperative-interaction-for-complementarity" class="md-nav__link">
    2.2.1 Cooperative Interaction for Complementarity
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#222-adversarial-interaction-for-advancement" class="md-nav__link">
    2.2.2 Adversarial Interaction for Advancement
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-interactive-engagement-between-human-and-agent" class="md-nav__link">
    2.3 Interactive Engagement between Human and Agent
  </a>
  
    <nav class="md-nav" aria-label="2.3 Interactive Engagement between Human and Agent">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#231-instructor-executor-paradigm" class="md-nav__link">
    2.3.1 Instructor-Executor Paradigm
  </a>
  
    <nav class="md-nav" aria-label="2.3.1 Instructor-Executor Paradigm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#education" class="md-nav__link">
    Education
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#health" class="md-nav__link">
    Health
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-application" class="md-nav__link">
    Other Application
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#232-equal-partnership-paradigm" class="md-nav__link">
    2.3.2 Equal Partnership Paradigm
  </a>
  
    <nav class="md-nav" aria-label="2.3.2 Equal Partnership Paradigm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#empathetic-communicator" class="md-nav__link">
    Empathetic Communicator
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#human-level-participant" class="md-nav__link">
    Human-Level Participant
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-agent-society-from-individuality-to-sociality" class="md-nav__link">
    3. Agent Society: From Individuality to Sociality
  </a>
  
    <nav class="md-nav" aria-label="3. Agent Society: From Individuality to Sociality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-behavior-and-personality-of-llm-based-agents" class="md-nav__link">
    3.1 Behavior and Personality of LLM-based Agents
  </a>
  
    <nav class="md-nav" aria-label="3.1 Behavior and Personality of LLM-based Agents">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#311-social-behavior" class="md-nav__link">
    3.1.1 Social Behavior
  </a>
  
    <nav class="md-nav" aria-label="3.1.1 Social Behavior">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#individual-behaviors" class="md-nav__link">
    Individual behaviors
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#group-behaviors" class="md-nav__link">
    Group behaviors
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#312-personality" class="md-nav__link">
    3.1.2 Personality
  </a>
  
    <nav class="md-nav" aria-label="3.1.2 Personality">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cognition" class="md-nav__link">
    Cognition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#emotion" class="md-nav__link">
    Emotion
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#character" class="md-nav__link">
    Character
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-environment-for-agent-society" class="md-nav__link">
    3.2 Environment for Agent Society
  </a>
  
    <nav class="md-nav" aria-label="3.2 Environment for Agent Society">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321-text-based-environment" class="md-nav__link">
    3.2.1 Text-based Environment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322-virtual-sandbox-environment" class="md-nav__link">
    3.2.2 Virtual Sandbox Environment
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323-physical-environment" class="md-nav__link">
    3.2.3 Physical Environment
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-society-simulation-with-llm-based-agents" class="md-nav__link">
    3.3 Society Simulation with LLM-based Agents
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>Embodied AI</h1>

<p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List">WooooDyy/LLM-Agent-Paper-List: The paper list of the 86-page paper "The Rise and Potential of Large Language Model Based Agents: A Survey" by Zhiheng Xi et al. (github.com)</a></p>
<h2 id="1-the-birth-of-an-agent-construction-of-llm-based-agents">1. The Birth of An Agent: Construction of LLM-based Agents</h2>
<p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure2.jpg"><img alt="" src="https://github.com/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure2.jpg" /></a></p>
<h3 id="11-brain-primarily-composed-of-an-llm"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#11-brain-primarily-composed-of-an-llm"></a>1.1 Brain: Primarily Composed of An LLM</h3>
<h4 id="111-natural-language-interaction"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#111-natural-language-interaction"></a>1.1.1 Natural Language Interaction</h4>
<h5 id="high-quality-generation"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#high-quality-generation"></a>High-quality generation</h5>
<ul>
<li>[2023/08] <strong>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.</strong> <em>Yejin Bang et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2302.04023">paper</a>]</li>
<li>[2023/06] <strong>LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models.</strong> <em>Yen-Ting Lin et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.13711">paper</a>]</li>
<li>[2023/04] <strong>Is ChatGPT a Highly Fluent Grammatical Error Correction System? A Comprehensive Evaluation.</strong> <em>Tao Fang et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2304.01746">paper</a>]</li>
</ul>
<h5 id="deep-understanding"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#deep-understanding"></a>Deep understanding</h5>
<ul>
<li>[2023/06] <strong>Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models.</strong> <em>Natalie Shapira et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.14763">paper</a>]</li>
<li>[2022/08] <strong>Inferring Rewards from Language in Context.</strong> <em>Jessy Lin et al. ACL.</em> [<a href="https://doi.org/10.18653/v1/2022.acl-long.585">paper</a>]</li>
<li>[2021/10] <strong>Theory of Mind Based Assistive Communication in Complex Human Robot Cooperation.</strong> <em>Moritz C. Buehler et al. arXiv.</em> [<a href="https://arxiv.org/abs/2109.01355">paper</a>]</li>
</ul>
<h4 id="112-knowledge"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#112-knowledge"></a>1.1.2 Knowledge</h4>
<h5 id="pretrain-model"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#pretrain-model"></a>Pretrain model</h5>
<ul>
<li>[2023/04] <strong>Learning Distributed Representations of Sentences from Unlabelled Data.</strong> <em>Felix Hill(University of Cambridge) et al. arXiv.</em> [<a href="https://arxiv.org/abs/1602.03483">paper</a>]</li>
<li>[2020/02] <strong>How Much Knowledge Can You Pack Into the Parameters of a Language Model?</strong> <em>Adam Roberts(Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2002.08910">paper</a>]</li>
<li>[2020/01] <strong>Scaling Laws for Neural Language Models.</strong> <em>Jared Kaplan(Johns Hopkins University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2001.08361">paper</a>]</li>
<li>[2017/12] <strong>Commonsense Knowledge in Machine Intelligence.</strong> <em>Niket Tandon(Allen Institute for Artificial Intelligence) et al. SIGMOD.</em> [<a href="https://sigmodrecord.org/publications/sigmodRecord/1712/pdfs/09_reports_Tandon.pdf">paper</a>]</li>
<li>[2011/03] <strong>Natural Language Processing (almost) from Scratch.</strong> <em>Ronan Collobert(Princeton) et al. arXiv.</em> [<a href="https://arxiv.org/abs/1103.0398">paper</a>]]</li>
</ul>
<h5 id="linguistic-knowledge"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#linguistic-knowledge"></a>Linguistic knowledge</h5>
<ul>
<li>[2023/02] <strong>A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity.</strong> <em>Yejin Bang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.04023">paper</a>]</li>
<li>[2021/06] <strong>Probing Pre-trained Language Models for Semantic Attributes and their Values.</strong> <em>Meriem Beloucif et al. EMNLP.</em> [<a href="https://aclanthology.org/2021.findings-emnlp.218/">paper</a>]</li>
<li>[2020/10] <strong>Probing Pretrained Language Models for Lexical Semantics.</strong> <em>Ivan Vulić et al. arXiv.</em> [<a href="https://arxiv.org/abs/2010.05731">paper</a>]</li>
<li>[2019/04] <strong>A Structural Probe for Finding Syntax in Word Representations.</strong> <em>John Hewitt et al. ACL.</em> [<a href="https://aclanthology.org/N19-1419/">paper</a>]</li>
<li>[2016/04] <strong>Improved Automatic Keyword Extraction Given More Semantic Knowledge.</strong> <em>H Leung. Systems for Advanced Applications.</em> [<a href="https://link.springer.com/chapter/10.1007/978-3-319-32055-7_10">paper</a>]</li>
</ul>
<h5 id="commonsense-knowledge"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#commonsense-knowledge"></a>Commonsense knowledge</h5>
<ul>
<li>[2022/10] <strong>Language Models of Code are Few-Shot Commonsense Learners.</strong> <em>Aman Madaan et al.arXiv.</em> [<a href="https://arxiv.org/abs/2210.07128">paper</a>]</li>
<li>[2021/04] <strong>Relational World Knowledge Representation in Contextual Language Models: A Review.</strong> <em>Tara Safavi et al. arXiv.</em> [<a href="https://arxiv.org/abs/2104.05837">paper</a>]</li>
<li>[2019/11] <strong>How Can We Know What Language Models Know?</strong> <em>Zhengbao Jiang et al.arXiv.</em> [<a href="https://arxiv.org/abs/1911.12543">paper</a>]</li>
</ul>
<h5 id="actionable-knowledge"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#actionable-knowledge"></a>Actionable knowledge</h5>
<ul>
<li>[2023/07] <strong>Large language models in medicine.</strong> <em>Arun James Thirunavukarasu et al. nature.</em> [<a href="https://www.nature.com/articles/s41591-023-02448-8">paper</a>]</li>
<li>[2023/06] <strong>DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation.</strong> <em>Yuhang Lai et al. ICML.</em> [<a href="https://proceedings.mlr.press/v202/lai23b.html">paper</a>]</li>
<li>[2022/10] <strong>Language Models of Code are Few-Shot Commonsense Learners.</strong> <em>Aman Madaan et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.07128">paper</a>]</li>
<li>[2022/02] <strong>A Systematic Evaluation of Large Language Models of Code.</strong> <em>Frank F. Xu et al.arXiv.</em> [<a href="https://arxiv.org/abs/2202.13169">paper</a>]</li>
<li>[2021/10] <strong>Training Verifiers to Solve Math Word Problems.</strong> <em>Karl Cobbe et al. arXiv.</em> [<a href="https://arxiv.org/abs/2110.14168">paper</a>]</li>
</ul>
<h5 id="potential-issues-of-knowledge"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#potential-issues-of-knowledge"></a>Potential issues of knowledge</h5>
<ul>
<li>[2023/05] <strong>Editing Large Language Models: Problems, Methods, and Opportunities.</strong> <em>Yunzhi Yao et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.13172">paper</a>]</li>
<li>[2023/05] <strong>Self-Checker: Plug-and-Play Modules for Fact-Checking with Large Language Models.</strong> <em>Miaoran Li et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14623">paper</a>]</li>
<li>[2023/05] <strong>CRITIC: Large Language Models Can Self-Correct with Tool-Interactive Critiquing.</strong> <em>Zhibin Gou et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11738">paper</a>]</li>
<li>[2023/04] <strong>Tool Learning with Foundation Models.</strong> <em>Yujia Qin et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.08354">paper</a>]</li>
<li>[2023/03] <strong>SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.</strong> <em>Potsawee Manakul et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.08896">paper</a>]</li>
<li>[2022/06] <strong>Memory-Based Model Editing at Scale.</strong> <em>Eric Mitchell et al. arXiv.</em> [<a href="https://arxiv.org/abs/2206.06520">paper</a>]</li>
<li>[2022/04] <strong>A Review on Language Models as Knowledge Bases.</strong> <em>Badr AlKhamissi et al.arXiv.</em> [<a href="https://arxiv.org/abs/2204.06031">paper</a>]</li>
<li>[2021/04] <strong>Editing Factual Knowledge in Language Models.</strong> <em>Nicola De Cao et al.arXiv.</em> [<a href="https://arxiv.org/abs/2104.08164">paper</a>]</li>
<li>[2017/08] <strong>Measuring Catastrophic Forgetting in Neural Networks.</strong> <em>Ronald Kemker et al.arXiv.</em> [<a href="https://arxiv.org/abs/1708.02072">paper</a>]</li>
</ul>
<h4 id="113-memory"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#113-memory"></a>1.1.3 Memory</h4>
<h5 id="memory-capability"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#memory-capability"></a>Memory capability</h5>
<h6 id="raising-the-length-limit-of-transformers"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#raising-the-length-limit-of-transformers"></a>Raising the length limit of Transformers</h6>
<ul>
<li>[2023/05] <strong>Randomized Positional Encodings Boost Length Generalization of Transformers.</strong> <em>Anian Ruoss (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16843">paper</a>] [<a href="https://github.com/google-deepmind/randomized_positional_encodings">code</a>]</li>
<li>[2023-03] <strong>CoLT5: Faster Long-Range Transformers with Conditional Computation.</strong> <em>Joshua Ainslie (Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.09752">paper</a>]</li>
<li>[2022/03] <strong>Efficient Classification of Long Documents Using Transformers.</strong> <em>Hyunji Hayley Park (Illinois University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2203.11258">paper</a>] [<a href="https://github.com/amazon-science/efficient-longdoc-classification">code</a>]</li>
<li>[2021/12] <strong>LongT5: Efficient Text-To-Text Transformer for Long Sequences.</strong> <em>Mandy Guo (Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.07916">paper</a>] [<a href="https://github.com/google-research/longt5">code</a>]</li>
<li>[2019/10] <strong>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.</strong> <em>Michael Lewis(Facebook AI) et al. arXiv.</em> [<a href="https://arxiv.org/abs/1910.13461">paper</a>] [<a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/bart">code</a>]</li>
</ul>
<h6 id="summarizing-memory"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#summarizing-memory"></a>Summarizing memory</h6>
<ul>
<li>[2023/08] <strong>ExpeL: LLM Agents Are Experiential Learners.</strong> <em>Andrew Zhao (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.10144">paper</a>] [code]</li>
<li>[2023/08] <strong>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.</strong> <em>Chi-Min Chan (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.07201">paper</a>] [<a href="https://github.com/thunlp/ChatEval">code</a>]</li>
<li>[2023/05] <strong>MemoryBank: Enhancing Large Language Models with Long-Term Memory.</strong> <em>Wanjun Zhong (Harbin Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10250">paper</a>] [<a href="https://github.com/zhongwanjun/memorybank-siliconfriend">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/04] <strong>Unleashing Infinite-Length Input Capacity for Large-scale Language Models with Self-Controlled Memory System.</strong> <em>Xinnian Liang(Beihang University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.13343">paper</a>] [<a href="https://github.com/wbbeyourself/scm4llms">code</a>]</li>
<li>[2023/03] <strong>Reflexion: Language Agents with Verbal Reinforcement Learning.</strong> <em>Noah Shinn (Northeastern University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.11366">paper</a>] [<a href="https://github.com/noahshinn024/reflexion">code</a>]</li>
</ul>
<h6 id="compressing-memories-with-vectors-or-data-structures"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#compressing-memories-with-vectors-or-data-structures"></a>Compressing memories with vectors or data structures</h6>
<ul>
<li>[2023/07] <strong>Communicative Agents for Software Development.</strong> <em>Chen Qian (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.07924">paper</a>] [<a href="https://github.com/openbmb/chatdev">code</a>]</li>
<li>[2023/06] <strong>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.</strong> <em>Chenxu Hu(Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.03901">paper</a>] [<a href="https://github.com/huchenxucs/ChatDB">code</a>]</li>
<li>[2023/05] <strong>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.</strong> <em>Xizhou Zhu (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17144">paper</a>] [<a href="https://github.com/OpenGVLab/GITM">code</a>]</li>
<li>[2023/05] <strong>RET-LLM: Towards a General Read-Write Memory for Large Language Models.</strong> <em>Ali Modarressi (LMU Munich) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14322">paper</a>] [<a href="https://github.com/tloen/alpaca-lora">code</a>]</li>
</ul>
<h5 id="memory-retrieval"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#memory-retrieval"></a>Memory retrieval</h5>
<ul>
<li>[2023/08] <strong>Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents.</strong> <em>Ziheng Huang(University of California—San Diego) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01542">paper</a>]</li>
<li>[2023/08] <strong>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.</strong> <em>Jiaju Lin (PTA Studio) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/06] <strong>ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory.</strong> <em>Chenxu Hu(Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.03901">paper</a>] [<a href="https://github.com/huchenxucs/ChatDB">code</a>]</li>
<li>[2023/05] <strong>MemoryBank: Enhancing Large Language Models with Long-Term Memory.</strong> <em>Wanjun Zhong (Harbin Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10250">paper</a>] [<a href="https://github.com/zhongwanjun/memorybank-siliconfriend">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
</ul>
<h4 id="114-reasoning-planning"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#114-reasoning--planning"></a>1.1.4 Reasoning &amp; Planning</h4>
<h5 id="reasoning"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#reasoning"></a>Reasoning</h5>
<ul>
<li>
<p>[2023/05] <strong>Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement.</strong> <em>Zhiheng Xi (Fudan University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14497">paper</a>] [<a href="https://github.com/woooodyy/self-polish">code</a>]</p>
</li>
<li>
<p>[2023-03] <strong>Large Language Models are Zero-Shot Reasoners.</strong> <em>Takeshi Kojima (The University of Tokyo) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.11916">paper</a>][<a href="https://github.com/kojima-takeshi188/zero_shot_cot">code</a>]</p>
</li>
<li>
<p>[2023/03] <strong>Self-Refine: Iterative Refinement with Self-Feedback.</strong> <em>Aman Madaan (Carnegie Mellon University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17651">paper</a>] [<a href="https://github.com/madaan/self-refine">code</a>]</p>
</li>
<li>
<p>[2022/05] <strong>Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning.</strong> <em>Antonia Creswell (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.09712">paper</a>]</p>
</li>
<li>
<p>[2022/03] <strong>Self-Consistency Improves Chain of Thought Reasoning in Language Models.</strong> <em>Xuezhi Wang(Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2203.11171">paper</a>] [<a href="https://github.com/huggingface/transformers/tree/main/src/transformers/models/bart">code</a>]</p>
</li>
<li>
<p>[2022/01] <strong>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.</strong> <em>Jason Wei (Google Research,) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2201.11903">paper</a>]</p>
</li>
</ul>
<h5 id="planning"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#planning"></a>Planning</h5>
<h6 id="plan-formulation"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#plan-formulation"></a>Plan formulation</h6>
<ul>
<li>[2023/05] <strong>Tree of Thoughts: Deliberate Problem Solving with Large Language Models.</strong> <em>Shunyu Yao (Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10601">paper</a>] [<a href="https://github.com/princeton-nlp/tree-of-thought-llm">code</a>]</li>
<li>[2023/05] <strong>Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.</strong> <em>Yue Wu(Carnegie Mellon University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.02412">paper</a>]</li>
<li>[2023/05] <strong>Reasoning with Language Model is Planning with World Model.</strong> <em>Shibo Hao (UC San Diego) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14992">paper</a>] [<a href="https://github.com/Ber666/RAP">code</a>]</li>
<li>[2023/05] <strong>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.</strong> <em>Bill Yuchen Lin (Allen Institute for Artificial Intelligence) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17390">paper</a>] [<a href="https://github.com/yuchenlin/swiftsage">code</a>]</li>
<li>[2023/04] <strong>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency.</strong> <em>Bo Liu (University of Texas at Austin) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.11477">paper</a>] [<a href="https://github.com/Cranial-XIX/llm-pddl">code</a>]</li>
<li>[2023/03] <strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.</strong> <em>Yongliang Shen (Microsoft Research Asia) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17580">paper</a>] [<a href="https://github.com/microsoft/JARVIS">code</a>]</li>
<li>[2023/02] <strong>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.</strong> <em>ZiHao Wang (Peking University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.01560">paper</a>] [<a href="https://github.com/CraftJarvis/MC-Planner">code</a>]</li>
<li>[2022/05] <strong>Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.</strong> <em>Denny Zhou (Google Research) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.10625">paper</a>]</li>
<li>[2022/05] <strong>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.</strong> <em>Ehud Karpas (AI21 Labs) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.00445">paper</a>]</li>
<li>[2022/04] <strong>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.</strong> <em>Michael Ahn (Robotics at Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2204.01691">paper</a>]</li>
</ul>
<h6 id="plan-reflection"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#plan-reflection"></a>Plan reflection</h6>
<ul>
<li>[2023/08] <strong>SelfCheck: Using LLMs to Zero-Shot Check Their Own Step-by-Step Reasoning.</strong> <em>Ning Miao (University of Oxford) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.00436">paper</a>] [<a href="https://github.com/NingMiao/SelfCheck">code</a>]</li>
<li>[2023/05] <strong>ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models.</strong> <em>Zhipeng Chen (Renmin University of China) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14323">paper</a>] [<a href="https://github.com/RUCAIBOX/ChatCoT">code</a>]</li>
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291">paper</a>] [<a href="https://voyager.minedojo.org/">code</a>]</li>
<li>[2023/03] <strong>Chat with the Environment: Interactive Multimodal Perception Using Large Language Models.</strong> <em>Xufeng Zhao (University Hamburg) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.08268">paper</a>] [<a href="https://matcha-model.github.io/">code</a>]</li>
<li>[2022/12] <strong>LLM-Planner: Few-Shot Grounded Planning for Embodied Agents with Large Language Models.</strong> <em>Chan Hee Song (The Ohio State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2212.04088">paper</a>] [<a href="https://dki-lab.github.io/LLM-Planner/">code</a>]</li>
<li>[2022/10] <strong>ReAct: Synergizing Reasoning and Acting in Language Models.</strong> <em>Shunyu Yao ( Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.03629">paper</a>] [<a href="https://react-lm.github.io/">code</a>]</li>
<li>[2022/07] <strong>Inner Monologue: Embodied Reasoning through Planning with Language Models.</strong> <em>Wenlong Huang (Robotics at Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.05608">paper</a>] [<a href="https://innermonologue.github.io/">code</a>]</li>
<li>[2021/10] <strong>AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts.</strong> <em>Tongshuang Wu (University of Washington) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2110.01691">paper</a>]</li>
</ul>
<h4 id="115-transferability-and-generalization"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#115-transferability-and-generalization"></a>1.1.5 Transferability and Generalization</h4>
<h5 id="unseen-task-generalization"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#unseen-task-generalization"></a>Unseen task generalization</h5>
<ul>
<li>[2023/05] <strong>Training language models to follow instructions with human feedback.</strong> <em>Long Ouyang et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html">paper</a>]</li>
<li>[2023/01] <strong>Multitask Prompted Training Enables Zero-Shot Task Generalization.</strong> <em>Victor Sanh et al. ICLR.</em> [<a href="https://openreview.net/forum?id=9Vrb9D0WI4">paper</a>]</li>
<li>[2022/10] <strong>Scaling Instruction-Finetuned Language Models.</strong> <em>Hyung Won Chung et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2210.11416">paper</a>]</li>
<li>[2022/08] <strong>Finetuned Language Models are Zero-Shot Learners.</strong> <em>Jason Wei et al. ICLR.</em> [<a href="https://openreview.net/forum?id=gEZrGCozdqR">paper</a>]</li>
</ul>
<h5 id="in-context-learning"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#in-context-learning"></a>In-context learning</h5>
<ul>
<li>[2023/08] <strong>Images Speak in Images: A Generalist Painter for In-Context Visual Learning.</strong> <em>Xinlong Wang et al. IEEE.</em> [<a href="https://doi.org/10.1109/CVPR52729.2023.00660">paper</a>]</li>
<li>[2023/08] <strong>Neural Codec Language Models are Zero-Shot Text to Speech Synthesizers.</strong> <em>Chengyi Wang et al. arXiv.</em> [<a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">paper</a>]</li>
<li>[2023/07] <strong>A Survey for In-context Learning.</strong> <em>Qingxiu Dong et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2301.00234">paper</a>]</li>
<li>[2023/05] <strong>Language Models are Few-Shot Learners.</strong> <em>Tom B. Brown (OpenAI) et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html">paper</a>]</li>
</ul>
<h5 id="continual-learning"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#continual-learning"></a>Continual learning</h5>
<ul>
<li>[2023/07] <strong>Progressive Prompts: Continual Learning for Language Models.</strong> <em>Razdaibiedina et al. arXiv.</em> [<a href="https://arxiv.org/abs/2301.12314">paper</a>]</li>
<li>[2023/07] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.16291">paper</a>]</li>
<li>[2023/01] <strong>A Comprehensive Survey of Continual Learning: Theory, Method and Application.</strong> <em>Liyuan Wang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.00487">paper</a>]</li>
<li>[2022/11] <strong>Continual Learning of Natural Language Processing Tasks: A Survey.</strong> <em>Zixuan Ke et al. arXiv.</em> [<a href="https://arxiv.org/abs/2211.12701">paper</a>]</li>
</ul>
<h3 id="12-perception-multimodal-inputs-for-llm-based-agents"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#12-perception-multimodal-inputs-for-llm-based-agents"></a>1.2 Perception: Multimodal Inputs for LLM-based Agents</h3>
<h4 id="121-visual"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#121-visual"></a>1.2.1 Visual</h4>
<ul>
<li>[2023/05] <strong>Language Is Not All You Need: Aligning Perception with Language Models.</strong> <em>Shaohan Huang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.14045">paper</a>]]</li>
<li>[2023/05] <strong>InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning.</strong> <em>Wenliang Dai et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.06500">paper</a>]</li>
<li>[2023/05] <strong>MultiModal-GPT: A Vision and Language Model for Dialogue with Humans.</strong> <em>Tao Gong et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.04790">paper</a>]</li>
<li>[2023/05] <strong>PandaGPT: One Model To Instruction-Follow Them All.</strong> <em>Yixuan Su et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16355">paper</a>]</li>
<li>[2023/04] <strong>Visual Instruction Tuning.</strong> <em>Haotian Liu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.08485">paper</a>]</li>
<li>[2023/04] <strong>MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.</strong> <em>Deyao Zhu. arXiv.</em> [<a href="https://arxiv.org/abs/2304.10592">paper</a>]</li>
<li>[2023/01] <strong>BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.</strong> <em>Junnan Li et al. arXiv.</em> [<a href="https://arxiv.org/abs/2301.12597">paper</a>]</li>
<li>[2022/04] <strong>Flamingo: a Visual Language Model for Few-Shot Learning.</strong> <em>Jean-Baptiste Alayrac et al. arXiv.</em> [<a href="https://arxiv.org/abs/2204.14198">paper</a>]</li>
<li>[2021/10] <strong>MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer.</strong> <em>Sachin Mehta et al.arXiv.</em> [<a href="https://arxiv.org/abs/2110.02178">paper</a>]</li>
<li>[2021/05] <strong>MLP-Mixer: An all-MLP Architecture for Vision.</strong> <em>Ilya Tolstikhin et al.arXiv.</em> [<a href="https://arxiv.org/abs/2105.01601">paper</a>]</li>
<li>[2020/10] <strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.</strong> <em>Alexey Dosovitskiy et al. arXiv.</em> [<a href="https://arxiv.org/abs/2010.11929">paper</a>]</li>
<li>[2017/11] <strong>Neural Discrete Representation Learning.</strong> <em>Aaron van den Oord et al. arXiv.</em> [<a href="https://arxiv.org/abs/1711.00937">paper</a>]</li>
</ul>
<h4 id="122-audio"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#122-audio"></a>1.2.2 Audio</h4>
<ul>
<li>[2023/06] <strong>Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding.</strong> <em>Hang Zhang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.02858">paper</a>]</li>
<li>[2023/05] <strong>X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages.</strong> <em>Feilong Chen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.04160">paper</a>]</li>
<li>[2023/05] <strong>InternGPT: Solving Vision-Centric Tasks by Interacting with ChatGPT Beyond Language.</strong> <em>Zhaoyang Liu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.05662">paper</a>]</li>
<li>[2023/04] <strong>AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head.</strong> <em>Rongjie Huang et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.12995">paper</a>]</li>
<li>[2023/03] <strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.</strong> <em>Yongliang Shen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17580">paper</a>]</li>
<li>[2021/06] <strong>HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units.</strong> <em>Wei-Ning Hsu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2106.07447">paper</a>]</li>
<li>[2021/04] <strong>AST: Audio Spectrogram Transformer.</strong> <em>Yuan Gong et al. arXiv.</em> [<a href="https://arxiv.org/abs/2104.01778">paper</a>]</li>
</ul>
<h3 id="13-action-expand-action-space-of-llm-based-agents"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#13-action-expand-action-space-of-llm-based-agents"></a>1.3 Action: Expand Action Space of LLM-based Agents</h3>
<h4 id="131-tool-using"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#131-tool-using"></a>1.3.1 Tool Using</h4>
<ul>
<li>[2023/07] <strong>ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs.</strong> <em>Yujia Qin et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.16789">paper</a>] [<a href="https://github.com/openbmb/toolbench">code</a>] [<a href="https://paperswithcode.com/dataset/toolbench">dataset</a>]</li>
<li>[2023/05] <strong>Large Language Models as Tool Makers.</strong> <em>Tianle Cai et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17126">paper</a>] [<a href="https://github.com/ctlllll/llm-toolmaker">code</a>]</li>
<li>[2023/05] <strong>CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation.</strong> <em>Cheng Qian et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14318">paper</a>]</li>
<li>[2023/04] <strong>Tool Learning with Foundation Models.</strong> <em>Yujia Qin et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.08354">paper</a>] [<a href="https://github.com/openbmb/bmtools">code</a>]</li>
<li>[2023/04] <strong>ChemCrow: Augmenting large-language models with chemistry tools.</strong> <em>Andres M Bran (Laboratory of Artificial Chemical Intelligence, ISIC, EPFL) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.05376">paper</a>] [<a href="https://github.com/ur-whitelab/chemcrow-public">code</a>]</li>
<li>[2023/04] <strong>GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information.</strong> <em>Qiao Jin, Yifan Yang, Qingyu Chen, Zhiyong Lu. arXiv.</em> [<a href="https://arxiv.org/abs/2304.09667">paper</a>] [<a href="https://github.com/ncbi/GeneGPT">code</a>]</li>
<li>[2023/04] <strong>OpenAGI: When LLM Meets Domain Experts.</strong> <em>Yingqiang Ge et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.04370">paper</a>] [<a href="https://github.com/agiresearch/openagi">code</a>]</li>
<li>[2023/03] <strong>HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face.</strong> <em>Yongliang Shen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17580">paper</a>] [<a href="https://github.com/microsoft/JARVIS">code</a>]</li>
<li>[2023/03] <strong>Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models.</strong> <em>Chenfei Wu et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.04671">paper</a>] [<a href="https://github.com/microsoft/visual-chatgpt">code</a>]</li>
<li>[2023/02] <strong>Augmented Language Models: a Survey.</strong> <em>Grégoire Mialon et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.07842">paper</a>]</li>
<li>[2023/02] <strong>Toolformer: Language Models Can Teach Themselves to Use Tools.</strong> <em>Timo Schick et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.04761">paper</a>]</li>
<li>[2022/05] <strong>TALM: Tool Augmented Language Models.</strong> <em>Aaron Parisi et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.12255">paper</a>]</li>
<li>[2022/05] <strong>MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.</strong> <em>Ehud Karpas et al. arXiv.</em> [<a href="https://arxiv.org/abs/2205.00445">paper</a>]</li>
<li>[2022/04] <strong>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.</strong> <em>Michael Ahn et al. arXiv.</em> [<a href="https://arxiv.org/abs/2204.01691">paper</a>]</li>
<li>[2021/12] <strong>WebGPT: Browser-assisted question-answering with human feedback.</strong> <em>Reiichiro Nakano et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.09332">paper</a>]</li>
<li>[2021/07] <strong>Evaluating Large Language Models Trained on Code.</strong> <em>Mark Chen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2107.03374">paper</a>] [<a href="https://github.com/openai/human-eval">code</a>]</li>
</ul>
<h4 id="132-embodied-action"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#132-embodied-action"></a>1.3.2 Embodied Action</h4>
<ul>
<li>[2023/07] <strong>Interactive language: Talking to robots in real time.</strong> <em>Corey Lynch et al. IEEE(RAL)</em> [<a href="https://arxiv.org/pdf/2210.06407.pdf">paper</a>]</li>
<li>[2023/05] <strong>Voyager: An open-ended embodied agent with large language models.</strong> <em>Guanzhi Wang et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2305.16291.pdf">paper</a>]</li>
<li>[2023/05] <strong>AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments.</strong> <em>Sudipta Paul et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/28f699175783a2c828ae74d53dd3da20-Paper-Conference.pdf">paper</a>]</li>
<li>[2023/05] <strong>EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought.</strong> <em>Yao Mu et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2305.15021.pdf">paper</a>] [<a href="https://github.com/EmbodiedGPT/EmbodiedGPT_Pytorch">code</a>]</li>
<li>[2023/05] <strong>NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models.</strong> <em>Gengze Zhou et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2305.16986.pdf">paper</a>]</li>
<li>[2023/05] <strong>AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation.</strong> <em>Chuhao Jin et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2305.18898.pdf">paper</a>]</li>
<li>[2023/03] <strong>PaLM-E: An Embodied Multimodal Language Model.</strong> <em>Danny Driess et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2303.03378.pdf">paper</a>]</li>
<li>[2023/03] <strong>Reflexion: Language Agents with Verbal Reinforcement Learning.</strong> <em>Noah Shinn et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2303.11366.pdf">paper</a>] [<a href="https://github.com/noahshinn024/reflexion">code</a>]</li>
<li>[2023/02] <strong>Collaborating with language models for embodied reasoning.</strong> <em>Ishita Dasgupta et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2302.00763.pdf">paper</a>]</li>
<li>[2023/02] <strong>Code as Policies: Language Model Programs for Embodied Control.</strong> <em>Jacky Liang et al. IEEE(ICRA).</em> [<a href="https://arxiv.org/pdf/2209.07753.pdf">paper</a>]</li>
<li>[2022/10] <strong>ReAct: Synergizing Reasoning and Acting in Language Models.</strong> <em>Shunyu Yao et al. Arxiv</em> [<a href="https://arxiv.org/pdf/2210.03629.pdf">paper</a>] [<a href="https://github.com/ysymyth/ReAct">code</a>]</li>
<li>[2022/10] <strong>Instruction-Following Agents with Multimodal Transformer.</strong> <em>Hao Liu et al. CVPR</em> [<a href="https://arxiv.org/pdf/2210.13431.pdf">paper</a>] [<a href="https://github.com/lhao499/instructrl">code</a>]</li>
<li>[2022/07] <strong>Inner Monologue: Embodied Reasoning through Planning with Language Models.</strong> <em>Wenlong Huang et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2207.05608.pdf">paper</a>]</li>
<li>[2022/07] <strong>LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action.</strong> <em>Dhruv Shahet al. CoRL</em> [<a href="https://proceedings.mlr.press/v205/shah23b/shah23b.pdf">paper</a>] [<a href="https://github.com/blazejosinski/lm_nav">code</a>]</li>
<li>[2022/04] <strong>Do As I Can, Not As I Say: Grounding Language in Robotic Affordances.</strong> <em>Michael Ahn et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2204.01691.pdf">paper</a>]</li>
<li>[2022/01] <strong>A Survey of Embodied AI: From Simulators to Research Tasks.</strong> <em>Jiafei Duan et al. IEEE(TETCI).</em> [<a href="https://arxiv.org/pdf/2103.04918.pdf">paper</a>]</li>
<li>[2022/01] <strong>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.</strong> <em>Wenlong Huang et al. Arxiv.</em> [<a href="https://arxiv.org/pdf/2201.07207v2.pdf">paper</a>] [<a href="https://github.com/huangwl18/language-planner">code</a>]</li>
<li>[2020/04] <strong>Experience Grounds Language.</strong> <em>Yonatan Bisk et al. EMNLP</em> [<a href="https://arxiv.org/pdf/2004.10151.pdf">paper</a>]</li>
<li>[2019/03] <strong>Review of Deep Reinforcement Learning for Robot Manipulation.</strong> <em>Hai Nguyen et al. IEEE(IRC).</em> [<a href="https://www.researchgate.net/profile/Hai-Nguyen-128/publication/355980729_Review_of_Deep_Reinforcement_Learning_for_Robot_Manipulation/links/6187ef153068c54fa5bb977e/Review-of-Deep-Reinforcement-Learning-for-Robot-Manipulation.pdf">paper</a>]</li>
<li>[2005/01] <strong>The Development of Embodied Cognition: Six Lessons from Babies.</strong> <em>Linda Smith et al. Artificial Life.</em> [<a href="https://cogdev.sitehost.iu.edu/labwork/6_lessons.pdf">paper</a>]</li>
</ul>
<h2 id="2-agents-in-practice-applications-of-llm-based-agents"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#2-agents-in-practice-applications-of-llm-based-agents"></a>2. Agents in Practice: Applications of LLM-based Agents</h2>
<p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure7.jpg"><img alt="" src="https://github.com/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure7.jpg" /></a></p>
<h3 id="21-general-ability-of-single-agent"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#21-general-ability-of-single-agent"></a>2.1 General Ability of Single Agent</h3>
<p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure8.jpg"><img alt="" src="https://github.com/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure8.jpg" /></a></p>
<h4 id="211-task-orietned-deployment"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#211-task-orietned-deployment"></a>2.1.1 Task-orietned Deployment</h4>
<p><strong>In web scenarios</strong></p>
<ul>
<li>[2023/07] <strong>WebArena: A Realistic Web Environment for Building Autonomous Agents.</strong> <em>Shuyan Zhou (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.13854">paper</a>] [<a href="https://webarena.dev/">code</a>]</li>
<li>[2023/07] <strong>A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis.</strong> <em>Izzeddin Gur (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.12856">paper</a>]</li>
<li>[2023/06] <strong>SYNAPSE: Leveraging Few-Shot Exemplars for Human-Level Computer Control.</strong> <em>Longtao Zheng (Nanyang Technological University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.07863">paper</a>] [<a href="https://github.com/ltzheng/synapse">code</a>]</li>
<li>[2023/06] <strong>Mind2Web: Towards a Generalist Agent for the Web.</strong> <em>Xiang Deng (The Ohio State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.06070">paper</a>] [<a href="https://osu-nlp-group.github.io/Mind2Web/">code</a>]</li>
<li>[2023/05] <strong>Multimodal Web Navigation with Instruction-Finetuned Foundation Models.</strong> <em>Hiroki Furuta (The University of Tokyo) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11854">paper</a>]</li>
<li>[2023/03] <strong>Language Models can Solve Computer Tasks.</strong> <em>Geunwoo Kim (University of California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17491">paper</a>] [<a href="https://github.com/posgnu/rci-agent">code</a>]</li>
<li>[2022/07] <strong>WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents.</strong> <em>Shunyu Yao (Princeton University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.01206">paper</a>] [<a href="https://webshop-pnlp.github.io/">code</a>]</li>
<li>[2021/12] <strong>WebGPT: Browser-assisted question-answering with human feedback.</strong> <em>Reiichiro Nakano (OpenAI) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2112.09332">paper</a>]</li>
</ul>
<p><strong>In life scenarios</strong></p>
<ul>
<li>[2023/08] <strong>InterAct: Exploring the Potentials of ChatGPT as a Cooperative Agent.</strong> <em>Po-Lin Chen et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01552">paper</a>]</li>
<li>[2023/05] <strong>Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents.</strong> <em>Yue Wu (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.02412">paper</a>]</li>
<li>[2023/05] <strong>Augmenting Autotelic Agents with Large Language Models.</strong> <em>Cédric Colas (MIT) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.12487">paper</a>]</li>
<li>[2023/03] <strong>Planning with Large Language Models via Corrective Re-prompting.</strong> <em>Shreyas Sundara Raman (Brown University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2211.09935">paper</a>]</li>
<li>[2022/10] <strong>Generating Executable Action Plans with Environmentally-Aware Language Models.</strong> <em>Maitrey Gramopadhye (University of North Carolina at Chapel Hill) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.04964">paper</a>] [<a href="https://github.com/hri-ironlab/scene_aware_language_planner">code</a>]</li>
<li>[2022/01] <strong>Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents.</strong> <em>Wenlong Huang (UC Berkeley) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2201.07207">paper</a>] [<a href="https://wenlong.page/language-planner/">code</a>]</li>
</ul>
<h4 id="212-innovation-oriented-deployment"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#212-innovation-oriented-deployment"></a>2.1.2 Innovation-oriented Deployment</h4>
<ul>
<li>[2023/08] <strong>The Hitchhiker's Guide to Program Analysis: A Journey with Large Language Models.</strong> <em>Haonan Li (UC Riverside) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.00245">paper</a>]</li>
<li>[2023/08] <strong>ChatMOF: An Autonomous AI System for Predicting and Generating Metal-Organic Frameworks.</strong> <em>Yeonghun Kang (Korea Advanced Institute of Science and Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01423">paper</a>]</li>
<li>[2023/07] <strong>Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics.</strong> <em>Melanie Swan (University College London) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.02502">paper</a>]</li>
<li>[2023/06] <strong>Towards Autonomous Testing Agents via Conversational Large Language Models.</strong> <em>Robert Feldt (Chalmers University of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.05152">paper</a>]</li>
<li>[2023/04] <strong>Emergent autonomous scientific research capabilities of large language models.</strong> <em>Daniil A. Boiko (CMU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.05332">paper</a>]</li>
<li>[2023/04] <strong>ChemCrow: Augmenting large-language models with chemistry tools.</strong> <em>Andres M Bran (Laboratory of Artificial Chemical Intelligence, ISIC, EPFL) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.05376">paper</a>] [<a href="https://github.com/ur-whitelab/chemcrow-public">code</a>]</li>
<li>[2022/03] <strong>ScienceWorld: Is your Agent Smarter than a 5th Grader?</strong> <em>Ruoyao Wang (University of Arizona) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2203.07540">paper</a>] [<a href="https://sciworld.apps.allenai.org/">code</a>]</li>
</ul>
<h4 id="213-lifecycle-oriented-deployment"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#213-lifecycle-oriented-deployment"></a>2.1.3 Lifecycle-oriented Deployment</h4>
<ul>
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291">paper</a>] [<a href="https://voyager.minedojo.org/">code</a>]</li>
<li>[2023/05] <strong>Ghost in the Minecraft: Generally Capable Agents for Open-World Environments via Large Language Models with Text-based Knowledge and Memory.</strong> <em>Xizhou Zhu (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17144">paper</a>] [<a href="https://github.com/OpenGVLab/GITM">code</a>]</li>
<li>[2023/03] <strong>Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks.</strong> <em>Haoqi Yuan (PKU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.16563">paper</a>] [<a href="https://sites.google.com/view/plan4mc">code</a>]</li>
<li>[2023/02] <strong>Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents.</strong> <em>Zihao Wang (PKU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.01560">paper</a>] [<a href="https://github.com/CraftJarvis/MC-Planner">code</a>]</li>
<li>[2023/01] <strong>Do Embodied Agents Dream of Pixelated Sheep: Embodied Decision Making using Language Guided World Modelling.</strong> <em>Kolby Nottingham (University of California Irvine, Irvine) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2301.12050">paper</a>] [<a href="https://deckardagent.github.io/">code</a>]</li>
</ul>
<h3 id="22-coordinating-potential-of-multiple-agents"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#22-coordinating-potential-of-multiple-agents"></a>2.2 Coordinating Potential of Multiple Agents</h3>
<p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure9.jpg"><img alt="" src="https://github.com/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure9.jpg" /></a></p>
<h4 id="221-cooperative-interaction-for-complementarity"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#221-cooperative-interaction-for-complementarity"></a>2.2.1 Cooperative Interaction for Complementarity</h4>
<p><strong>Disordered cooperation</strong></p>
<ul>
<li>[2023/07] <strong>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration.</strong> <em>Zhenhailong Wang (University of Illinois Urbana-Champaign) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.05300">paper</a>] [<a href="https://github.com/MikeWangWZHL/Solo-Performance-Prompting">code</a>]</li>
<li>[2023/07] <strong>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models.</strong> <em>Zhao Mandi, Shreeya Jain, Shuran Song (Columbia University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.04738">paper</a>] [<a href="https://project-roco.github.io/">code</a>]</li>
<li>[2023/04] <strong>ChatLLM Network: More brains, More intelligence.</strong> <em>Rui Hao (Beijing University of Posts and Telecommunications) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.12998">paper</a>]</li>
<li>[2023/01] <strong>Blind Judgement: Agent-Based Supreme Court Modelling With GPT.</strong> <em>Sil Hamilton (McGill University). arXiv.</em> [<a href="https://arxiv.org/abs/2301.05327">paper</a>]</li>
</ul>
<p><strong>Ordered cooperation</strong></p>
<ul>
<li>[2023/08] <strong>CGMI: Configurable General Multi-Agent Interaction Framework.</strong> <em>Shi Jinxin (East China Normal University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.12503">paper</a>]</li>
<li>[2023/08] <strong>ProAgent: Building Proactive Cooperative AI with Large Language Models.</strong> <em>Ceyao Zhang (The Chinese University of Hong Kong, Shenzhen) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.11339">paper</a>] [<a href="https://pku-proagent.github.io/">code</a>]</li>
<li>[2023/08] <strong>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents.</strong> <em>Weize Chen (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.10848">paper</a>] [<a href="https://github.com/OpenBMB/AgentVerse">code</a>]</li>
<li>[2023/08] <strong>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.</strong> <em>Qingyun Wu (Pennsylvania State University ) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.08155">paper</a>] [<a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Autogen/">code</a>]</li>
<li>[2023/08] <strong>MetaGPT: Meta Programming for Multi-Agent Collaborative Framework.</strong> <em>Sirui Hong (DeepWisdom) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.00352">paper</a>] [<a href="https://github.com/geekan/MetaGPT">code</a>]</li>
<li>[2023/07] <strong>Communicative Agents for Software Development.</strong> <em>Chen Qian (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.07924">paper</a>] [<a href="https://github.com/openbmb/chatdev">code</a>]</li>
<li>[2023/06] <strong>Multi-Agent Collaboration: Harnessing the Power of Intelligent LLM Agents.</strong> <em>Yashar Talebira (University of Alberta) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.03314">paper</a>]</li>
<li>[2023/05] <strong>Training Socially Aligned Language Models in Simulated Human Society.</strong> <em>Ruibo Liu (Dartmouth College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16960">paper</a>] [<a href="https://github.com/agi-templar/Stable-Alignment">code</a>]</li>
<li>[2023/05] <strong>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks.</strong> <em>Bill Yuchen Lin (Allen Institute for Artificial Intelligence) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.17390">paper</a>] [<a href="https://yuchenlin.xyz/swiftsage/">code</a>]</li>
<li>[2023/05] <strong>ChatGPT as your Personal Data Scientist.</strong> <em>Md Mahadi Hassan (Auburn University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.13657">paper</a>]</li>
<li>[2023/03] <strong>CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society.</strong> <em>Guohao Li (King Abdullah University of Science and Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17760">paper</a>] [<a href="https://github.com/lightaime/camel">code</a>]</li>
<li>[2023/03] <strong>DERA: Enhancing Large Language Model Completions with Dialog-Enabled Resolving Agents.</strong> <em>Varun Nair (Curai Health) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17071">paper</a>] [<a href="https://github.com/curai/curai-research/tree/main/DERA">code</a>]</li>
</ul>
<h4 id="222-adversarial-interaction-for-advancement"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#222-adversarial-interaction-for-advancement"></a>2.2.2 Adversarial Interaction for Advancement</h4>
<ul>
<li>[2023/08] <strong>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.</strong> <em>Chi-Min Chan (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.07201">paper</a>] [<a href="https://github.com/thunlp/ChatEval">code</a>]</li>
<li>[2023/05] <strong>Improving Factuality and Reasoning in Language Models through Multiagent Debate.</strong> <em>Yilun Du (MIT CSAIL) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.14325">paper</a>] [<a href="https://composable-models.github.io/llm_debate/">code</a>]</li>
<li>[2023/05] <strong>Improving Language Model Negotiation with Self-Play and In-Context Learning from AI Feedback.</strong> <em>Yao Fu (University of Edinburgh) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.10142">paper</a>] [<a href="https://github.com/FranxYao/GPT-Bargaining">code</a>]</li>
<li>[2023/05] <strong>Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate.</strong> <em>Kai Xiong (Harbin Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.11595">paper</a>] [<a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main">code</a>]</li>
<li>[2023/05] <strong>Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate.</strong> <em>Tian Liang (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.19118">paper</a>] [<a href="https://github.com/Skytliang/Multi-Agents-Debate">code</a>]</li>
</ul>
<h3 id="23-interactive-engagement-between-human-and-agent"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#23-interactive-engagement-between-human-and-agent"></a>2.3 Interactive Engagement between Human and Agent</h3>
<p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure10.jpg"><img alt="" src="https://github.com/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure10.jpg" /></a></p>
<h4 id="231-instructor-executor-paradigm"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#231-instructor-executor-paradigm"></a>2.3.1 Instructor-Executor Paradigm</h4>
<h5 id="education"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#education"></a>Education</h5>
<ul>
<li>[2023/07] <strong>Math Agents: Computational Infrastructure, Mathematical Embedding, and Genomics.</strong> <em>Melanie Swan (UCL) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2307.02502">paper</a>]<ul>
<li>Communicate with humans to help them understand and use mathematics.</li>
</ul>
</li>
<li>[2023/03] <strong>Hey Dona! Can you help me with student course registration?</strong> <em>Vishesh Kalvakurthi (MSU) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2303.13548">paper</a>]<ul>
<li>This is a developed application called Dona that offers virtual voice assistance in student course registration, where humans provide instructions.</li>
</ul>
</li>
</ul>
<h5 id="health"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#health"></a>Health</h5>
<ul>
<li>[2023/08] <strong>Zhongjing: Enhancing the Chinese Medical Capabilities of Large Language Model through Expert Feedback and Real-world Multi-turn Dialogue.</strong> <em>Songhua Yang (ZZU) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2308.03549">paper</a>] [<a href="https://github.com/SupritYoung/Zhongjing">code</a>]</li>
<li>[2023/05] <strong>HuatuoGPT, towards Taming Language Model to Be a Doctor.</strong> <em>Hongbo Zhang (CUHK-SZ) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.15075">paper</a>] [<a href="https://github.com/FreedomIntelligence/HuatuoGPT">code</a>] [<a href="https://www.huatuogpt.cn/">demo</a>]</li>
<li>[2023/05] <strong>Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback.</strong> <em>Shang-Ling Hsu (Gatech) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.08982">paper</a>]</li>
<li>[2020/10] <strong>A Virtual Conversational Agent for Teens with Autism Spectrum Disorder: Experimental Results and Design Lessons.</strong> <em>Mohammad Rafayet Ali (U of R) et al. IVA '20.</em> [<a href="https://doi.org/10.1145/3383652.3423900">paper</a>]</li>
</ul>
<h5 id="other-application"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#other-application"></a>Other Application</h5>
<ul>
<li>[2023/08] <strong>RecMind: Large Language Model Powered Agent For Recommendation.</strong> <em>Yancheng Wang (ASU, Amazon) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2308.14296">paper</a>]</li>
<li>[2023/08] <strong>Multi-Turn Dialogue Agent as Sales' Assistant in Telemarketing.</strong> <em>Wanting Gao (JNU) et al. IEEE.</em> [<a href="https://doi.org/10.1109/IJCNN54540.2023.10192042">paper</a>]</li>
<li>[2023/07] <strong>PEER: A Collaborative Language Model.</strong> <em>Timo Schick (Meta AI) et al. arXiv.</em> [<a href="https://openreview.net/pdf?id=KbYevcLjnc">paper</a>]</li>
<li>[2023/07] <strong>DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations.</strong> <em>Bo-Ru Lu (UW) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2307.07047">paper</a>]</li>
<li>[2023/06] <strong>AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn.</strong> <em>Difei Gao (NUS) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2306.08640">paper</a>]</li>
</ul>
<h4 id="232-equal-partnership-paradigm"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#232-equal-partnership-paradigm"></a>2.3.2 Equal Partnership Paradigm</h4>
<h5 id="empathetic-communicator"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#empathetic-communicator"></a>Empathetic Communicator</h5>
<ul>
<li>[2023/08] <strong>SAPIEN: Affective Virtual Agents Powered by Large Language Models.</strong> <em>Masum Hasan et al. arXiv.</em> [<a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">paper</a>] [<a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">code</a>] [<a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">project page</a>] [<a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/xx">dataset</a>]</li>
<li>[2023/05] <strong>Helping the Helper: Supporting Peer Counselors via AI-Empowered Practice and Feedback.</strong> <em>Shang-Ling Hsu (Gatech) et al. arXiv.</em> [<a href="https://doi.org/10.48550/arXiv.2305.08982">paper</a>]</li>
<li>[2022/07] <strong>Artificial empathy in marketing interactions: Bridging the human-AI gap in affective and social customer experience.</strong> <em>Yuping Liu‑Thompkins et al.</em> [<a href="https://link.springer.com/article/10.1007/s11747-022-00892-5">paper</a>]</li>
</ul>
<h5 id="human-level-participant"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#human-level-participant"></a>Human-Level Participant</h5>
<ul>
<li>[2023/08] <strong>Quantifying the Impact of Large Language Models on Collective Opinion Dynamics.</strong> <em>Chao Li et al. CoRR.</em> [<a href="https://doi.org/10.48550/arXiv.2308.03313">paper</a>]</li>
<li>[2023/06] <strong>Mastering the Game of No-Press Diplomacy via Human-Regularized Reinforcement Learning and Planning.</strong> <em>Anton Bakhtin et al. ICLR.</em> [<a href="https://openreview.net/pdf?id=F61FwJTZhb">paper</a>]</li>
<li>[2023/06] <strong>Decision-Oriented Dialogue for Human-AI Collaboration.</strong> <em>Jessy Lin et al. CoRR.</em> [<a href="https://doi.org/10.48550/arXiv.2305.20076">paper</a>]</li>
<li>[2022/11] <strong>Human-level play in the game of Diplomacy by combining language models with strategic reasoning.</strong> <em>FAIR et al. Science.</em> [<a href="https://www.science.org/doi/10.1126/science.ade9097">paper</a>]</li>
</ul>
<h2 id="3-agent-society-from-individuality-to-sociality"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#3-agent-society-from-individuality-to-sociality"></a>3. Agent Society: From Individuality to Sociality</h2>
<p><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List/blob/main/assets/figure12.jpg"><img alt="" src="https://github.com/WooooDyy/LLM-Agent-Paper-List/raw/main/assets/figure12.jpg" /></a></p>
<h3 id="31-behavior-and-personality-of-llm-based-agents"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#31-behavior-and-personality-of-llm-based-agents"></a>3.1 Behavior and Personality of LLM-based Agents</h3>
<h4 id="311-social-behavior"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#311-social-behavior"></a>3.1.1 Social Behavior</h4>
<h5 id="individual-behaviors"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#individual-behaviors"></a>Individual behaviors</h5>
<ul>
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291">paper</a>] [<a href="https://voyager.minedojo.org/">code</a>]</li>
<li>[2023/04] <strong>LLM+P: Empowering Large Language Models with Optimal Planning Proficiency.</strong> <em>Bo Liu (University of Texas) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.11477">paper</a>] [<a href="https://github.com/Cranial-XIX/llm-pddl">code</a>]</li>
<li>[2023/03] <strong>Reflexion: Language Agents with Verbal Reinforcement Learning.</strong> <em>Noah Shinn (Northeastern University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.11366">paper</a>] [<a href="https://github.com/noahshinn024/reflexion">code</a>]</li>
<li>[2023/03] <strong>PaLM-E: An Embodied Multimodal Language Model.</strong> <em>Danny Driess (Google) et al. ICML.</em> [<a href="http://proceedings.mlr.press/v202/driess23a/driess23a.pdf">paper</a>] [<a href="https://palm-e.github.io/">project page</a>]</li>
<li>[2023/03] <strong>ReAct: Synergizing Reasoning and Acting in Language Models.</strong> <em>Shunyu Yao (Princeton University) et al. ICLR.</em> [<a href="https://openreview.net/pdf?id=WE_vluYUL-X">paper</a>] [<a href="https://react-lm.github.io/">project page</a>]</li>
<li>[2022/01] <strong>Chain-of-thought prompting elicits reasoning in large language models.</strong> <em>Jason Wei (Google) et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf">paper</a>]</li>
</ul>
<h5 id="group-behaviors"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#group-behaviors"></a>Group behaviors</h5>
<ul>
<li>
<p>[2023/09] <strong>Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf.</strong> <em>Yuzhuang Xu (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2309.04658">paper</a>]</p>
</li>
<li>
<p>[2023/08] <strong>AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors in Agents.</strong> <em>Weize Chen (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.10848">paper</a>] [<a href="https://github.com/OpenBMB/AgentVerse">code</a>]</p>
</li>
<li>
<p>[2023/08] <strong>AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework.</strong> <em>Qingyun Wu (Pennsylvania State University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.08155">paper</a>] [<a href="https://microsoft.github.io/FLAML/docs/Use-Cases/Autogen/">code</a>]</p>
</li>
<li>
<p>[2023/08] <strong>ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate.</strong> <em>Chi-Min Chan (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.07201">paper</a>] [<a href="https://github.com/thunlp/ChatEval">code</a>]</p>
</li>
<li>
<p>[2023/07] <strong>Communicative Agents for Software Development.</strong> <em>Chen Qian (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.07924">paper</a>] [<a href="https://github.com/openbmb/chatdev">code</a>]</p>
</li>
<li>
<p>[2023/07] <strong>RoCo: Dialectic Multi-Robot Collaboration with Large Language Models.</strong> <em>Zhao Mandi, Shreeya Jain, Shuran Song (Columbia University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.04738">paper</a>] [<a href="https://project-roco.github.io/">code</a>]</p>
</li>
<li>
<p>[2023/08] <strong>ProAgent: Building Proactive Cooperative AI with Large Language Models.</strong> <em>Ceyao Zhang (The Chinese University of Hong Kong, Shenzhen) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.11339">paper</a>] [<a href="https://pku-proagent.github.io/">code</a>]</p>
</li>
</ul>
<h4 id="312-personality"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#312-personality"></a>3.1.2 Personality</h4>
<h5 id="cognition"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#cognition"></a>Cognition</h5>
<ul>
<li>[2023/03] <strong>Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods.</strong> <em>Thilo Hagendorff (University of Stuttgart) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.13988">paper</a>]</li>
<li>[2023/03] <strong>Mind meets machine: Unravelling GPT-4's cognitive psychology.</strong> <em>Sifatkaur Dhingra (Nowrosjee Wadia College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.11436">paper</a>]</li>
<li>[2022/07] <strong>Language models show human-like content effects on reasoning.</strong> <em>Ishita Dasgupta (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2207.07051">paper</a>]</li>
<li>[2022/06] <strong>Using cognitive psychology to understand GPT-3.</strong> <em>Marcel Binz et al. arXiv.</em> [<a href="https://arxiv.org/abs/2206.14576">paper</a>]</li>
</ul>
<h5 id="emotion"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#emotion"></a>Emotion</h5>
<ul>
<li>[2023/07] <strong>Emotional Intelligence of Large Language Models.</strong> <em>Xuena Wang (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.09042">paper</a>]</li>
<li>[2023/05] <strong>ChatGPT outperforms humans in emotional awareness evaluations.</strong> <em>Zohar Elyoseph et al. Frontiers in Psychology.</em> [<a href="https://www.frontiersin.org/articles/10.3389/fpsyg.2023.1199058/full">paper</a>]</li>
<li>[2023/02] <strong>Empathetic AI for Empowering Resilience in Games.</strong> <em>Reza Habibi (University of California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2302.09070">paper</a>]</li>
<li>[2022/12] <strong>Computer says “No”: The Case Against Empathetic Conversational AI.</strong> <em>Alba Curry (University of Leeds) et al. ACL.</em> [<a href="https://aclanthology.org/2023.findings-acl.515.pdf">paper</a>]</li>
</ul>
<h5 id="character"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#character"></a>Character</h5>
<ul>
<li>[2023/07] <strong>Do LLMs Possess a Personality? Making the MBTI Test an Amazing Evaluation for Large Language Models.</strong> <em>Keyu Pan (ByteDance) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.16180">paper</a>] [<a href="https://github.com/HarderThenHarder/transformers_tasks">code</a>]</li>
<li>[2023/07] <strong>Personality Traits in Large Language Models.</strong> <em>Mustafa Safdari (DeepMind) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.00184">paper</a>] [<a href="https://github.com/HarderThenHarder/transformers_tasks">code</a>]</li>
<li>[2022/12] <strong>Does GPT-3 Demonstrate Psychopathy? Evaluating Large Language Models from a Psychological Perspective.</strong> <em>Xingxuan Li (Alibaba) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2212.10529">paper</a>]</li>
<li>[2022/12] <strong>Identifying and Manipulating the Personality Traits of Language Models.</strong> <em>Graham Caron et al. arXiv.</em> [<a href="https://arxiv.org/abs/2212.10276">paper</a>]</li>
</ul>
<h3 id="32-environment-for-agent-society"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#32-environment-for-agent-society"></a>3.2 Environment for Agent Society</h3>
<h4 id="321-text-based-environment"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#321-text-based-environment"></a>3.2.1 Text-based Environment</h4>
<ul>
<li>[2023/08] <strong>Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models.</strong> <em>Aidan O’Gara (University of Southern California) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2308.01404">paper</a>] [<a href="https://github.com/aogara-ds/hoodwinked">code</a>]</li>
<li>[2023/03] <strong>CAMEL: Communicative Agents for "Mind" Exploration of Large Scale Language Model Society.</strong> <em>Guohao Li (King Abdullah University of Science and Technology) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.17760">paper</a>] [<a href="https://github.com/lightaime/camel">code</a>]</li>
<li>[2020/12] <strong>Playing Text-Based Games with Common Sense.</strong> <em>Sahith Dambekodi (Georgia Institute of Technology) et al. arXiv.</em> [<a href="https://arxiv.org/pdf/2012.02757.pdf">paper</a>]</li>
<li>[2019/09] <strong>Interactive Fiction Games: A Colossal Adventure.</strong> <em>Matthew Hausknecht (Microsoft Research) et al. AAAI.</em> [<a href="https://cdn.aaai.org/ojs/6297/6297-13-9522-1-10-20200516.pdf">paper</a>] [<a href="https://github.com/microsoft/jericho">code</a>]</li>
<li>[2019/03] <strong>Learning to Speak and Act in a Fantasy Text Adventure Game.</strong> <em>Jack Urbanek (Facebook) et al. ACL.</em> [<a href="https://aclanthology.org/D19-1062.pdf">paper</a>] [<a href="https://parl.ai/projects/light/">code</a>]</li>
<li>[2018/06] <strong>TextWorld: A Learning Environment for Text-based Games.</strong> <em>Marc-Alexandre Côté (Microsoft Research) et al. IJCAI.</em> [<a href="https://link.springer.com/chapter/10.1007/978-3-030-24337-1_3">paper</a>] [<a href="https://github.com/Microsoft/TextWorld">code</a>]</li>
</ul>
<h4 id="322-virtual-sandbox-environment"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#322-virtual-sandbox-environment"></a>3.2.2 Virtual Sandbox Environment</h4>
<ul>
<li>[2023/08] <strong>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.</strong> <em>Jiaju Lin (PTA Studio) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/05] <strong>Training Socially Aligned Language Models in Simulated Human Society.</strong> <em>Ruibo Liu (Dartmouth College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16960">paper</a>] [<a href="https://github.com/agi-templar/Stable-Alignment">code</a>]</li>
<li>[2023/05] <strong>Voyager: An Open-Ended Embodied Agent with Large Language Models.</strong> <em>Guanzhi Wang (NVIDA) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16291">paper</a>] [<a href="https://voyager.minedojo.org/">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/03] <strong>Plan4MC: Skill Reinforcement Learning and Planning for Open-World Minecraft Tasks.</strong> <em>Haoqi Yuan (PKU) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2303.16563">paper</a>] [<a href="https://sites.google.com/view/plan4mc">code</a>]</li>
<li>[2022/06] <strong>MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge.</strong> <em>Linxi Fan (NVIDIA) et al. NeurIPS.</em> [<a href="https://papers.nips.cc/paper_files/paper/2022/file/74a67268c5cc5910f64938cac4526a90-Paper-Datasets_and_Benchmarks.pdf">paper</a>] [<a href="https://minedojo.org/">project page</a>]</li>
</ul>
<h4 id="323-physical-environment"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#323-physical-environment"></a>3.2.3 Physical Environment</h4>
<ul>
<li>[2023/09] <strong>RoboAgent: Generalization and Efficiency in Robot Manipulation via Semantic Augmentations and Action Chunking.</strong> <em>Homanga Bharadhwaj (Carnegie Mellon University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2309.01918">paper</a>] [<a href="https://robopen.github.io/">project page</a>]</li>
<li>[2023/05] <strong>AVLEN: Audio-Visual-Language Embodied Navigation in 3D Environments.</strong> <em>Sudipta Paul et al. NeurIPS.</em> [<a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/28f699175783a2c828ae74d53dd3da20-Paper-Conference.pdf">paper</a>]</li>
<li>[2023/03] <strong>PaLM-E: An Embodied Multimodal Language Model.</strong> <em>Danny Driess (Google) et al. ICML.</em> [<a href="http://proceedings.mlr.press/v202/driess23a/driess23a.pdf">paper</a>] [<a href="https://palm-e.github.io/">project page</a>]</li>
<li>[2022/10] <strong>Interactive Language: Talking to Robots in Real Time.</strong> <em>Corey Lynch (Google) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2210.06407">paper</a>] [<a href="https://github.com/google-research/language-table">code</a>]</li>
</ul>
<h3 id="33-society-simulation-with-llm-based-agents"><a href="https://github.com/WooooDyy/LLM-Agent-Paper-List#33-society-simulation-with-llm-based-agents"></a>3.3 Society Simulation with LLM-based Agents</h3>
<ul>
<li>[2023/08] <strong>AgentSims: An Open-Source Sandbox for Large Language Model Evaluation.</strong> <em>Jiaju Lin (PTA Studio) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2023/07] <strong>S$^3$ : Social-network Simulation System with Large Language Model-Empowered Agents.</strong> <em>Chen Gao (Tsinghua University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.14984">paper</a>]</li>
<li>[2023/07] <strong>Epidemic Modeling with Generative Agents.</strong> <em>Ross Williams (Virginia Tech) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2307.04986">paper</a>] [<a href="https://github.com/bear96/GABM-Epidemic">code</a>]</li>
<li>[2023/06] <strong>RecAgent: A Novel Simulation Paradigm for Recommender Systems.</strong> <em>Lei Wang (Renmin University of China) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2306.02552">paper</a>]</li>
<li>[2023/05] <strong>Training Socially Aligned Language Models in Simulated Human Society.</strong> <em>Ruibo Liu (Dartmouth College) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2305.16960">paper</a>] [<a href="https://github.com/agi-templar/Stable-Alignment">code</a>]</li>
<li>[2023/04] <strong>Generative Agents: Interactive Simulacra of Human Behavior.</strong> <em>Joon Sung Park (Stanford University) et al. arXiv.</em> [<a href="https://arxiv.org/abs/2304.03442">paper</a>] [<a href="https://github.com/joonspk-research/generative_agents">code</a>]</li>
<li>[2022/08] <strong>Social Simulacra: Creating Populated Prototypes for Social Computing Systems.</strong> <em>Joon Sung Park (Stanford University) et al. UIST.</em> [<a href="https://dl.acm.org/doi/10.1145/3526113.3545616">paper</a>]</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../../..", "features": [], "search": "../../../assets/javascripts/workers/search.74e28a9f.min.js", "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.220ee61c.min.js"></script>
      
    
  </body>
</html>